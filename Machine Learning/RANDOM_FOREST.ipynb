{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RANDOM_FOREST.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"KiuZiGM6UYVc"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5LUGO9N_WkIM"},"source":["data= pd.read_json(\"/content/drive/My Drive/Fake news pro/1th Milestone/DATA/persian subjective tweets/LABELED.json\",orient=\"split\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4bV7rUf6mBk"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uW7OxMw-WtHg","outputId":"b3a7b083-d15e-4d3e-c93e-19cd0824aef4","colab":{"base_uri":"https://localhost:8080/","height":91},"executionInfo":{"status":"ok","timestamp":1585831421701,"user_tz":-270,"elapsed":2597,"user":{"displayName":"Milad Ranjbar","photoUrl":"","userId":"00501567754008442254"}}},"source":["import re\n","def pre_process(text):\n","    \n","    # lowercase\n","    text=text.lower()\n","    \n","    #remove tags\n","    text=re.sub(\"<!--?.*?-->\",\"\",text)\n","    \n","    # remove special characters and digits\n","    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n","    \n","    return text\n"," \n","# data['text'] = df_idf['title'] + df_idf['body']\n","data['text'] = data['text'].apply(lambda x:pre_process(x))\n","\n","def get_stop_words(stop_file_path):\n","\n","    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n","        stopwords = f.readlines()\n","        stop_set = set(m.strip() for m in stopwords)\n","        return frozenset(stop_set)\n"," \n","#load a set of stop words\n","stopwords=get_stop_words(\"/content/drive/My Drive/Fake news pro/step2.1/stopwords.dat\")\n","print(stopwords) \n","#get the text column \n","docs=data['text'].tolist()\n"," \n","cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n","word_count_vector=cv.fit_transform(docs)\n"," \n","from sklearn.feature_extraction.text import TfidfTransformer\n","tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n","tfidf_transformer.fit(word_count_vector)\n","x = tfidf_transformer.fit_transform(word_count_vector)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["frozenset({'نسبت', 'زیر', 'کافی', 'گاه', 'مورد', 'یک', 'داشته_است', 'اول', 'هنوز', 'متاسفانه', 'افزود', 'آمده_است', 'باشد', 'جدید', 'آوری', 'لحاظ', 'نه', 'کنار', 'ندارند', 'طول', 'ریزی', 'سه', 'بر', 'بین', 'کرد', 'خواهد_شد', 'روبه', 'می\\u200cکنند', 'نباید', 'پنج', 'آنکه', 'اینجا', 'می\\u200cرود', 'داشت', 'ما', 'چنین', 'هیچ', 'بالا', 'سال\\u200cهای', 'کل', 'کدام', 'می\\u200cتوانند', 'داشتن', 'حالا', 'خواهد_کرد', 'افراد', 'می\\u200cکردند', 'فردی', 'سپس', 'وگو', 'بخشی', 'بعضی', 'فقط', 'روی', 'کند', 'می\\u200cیابد', 'آنان', 'یعنی', 'خود', 'برای', 'نیمه', 'حداقل', 'علت', 'زیاد', 'یا', 'مثل', 'نیستند', 'چیز', 'می\\u200cکرد', 'خصوص', 'گیری', 'فکر', 'آقای', 'داشته_باشد', 'جدی', 'اینکه', 'می\\u200cکند', 'حد', 'داده', 'با', 'شامل', 'گرفت', 'وی', 'آن', 'موجب', 'شما', 'دیگر', 'مربوط', 'دانست', 'کننده', 'گردد', 'بیشتر', 'روش', 'متفاوت', 'راه', 'تبدیل', 'می\\u200cشوند', 'دوباره', 'جز', 'گفته', 'تمام', 'سایر', 'بی', 'دور', 'باز', 'نبود', 'کرده_اند', 'بلکه', 'می\\u200cباشد', 'بسیاری', 'جای', 'ناشی', 'داشتند', 'پشت', 'خواهد_بود', 'نوعی', 'حتی', 'خویش', 'بهترین', 'شخصی', 'البته', 'مدت', 'دارد', 'سراسر', 'جاری', 'بدون', 'آنها', 'آنجا', 'برابر', 'بار', 'نخستین', 'میان', 'علاوه', 'سهم', 'باید', 'کردن', 'شده_است', 'باره', 'ابتدا', 'بودند', 'گرفته', 'یکدیگر', 'گذاری', 'ولی', 'می\\u200cشد', 'حال', 'بزرگ', 'گیرد', 'تحت', 'و', 'را', 'دیگری', 'سوم', 'دهد', 'جریان', 'همچنان', 'وقتی', 'داریم', 'چرا', 'زیرا', 'مواجه', 'شمار', 'اش', 'هر', 'کنند', 'نشست', 'هنگام', 'تاکنون', 'می', 'ضمن', 'رسید', 'شوند', 'طور', 'خیلی', 'پخش', 'عالی', 'شود', 'می\\u200cآید', 'هایی', 'خودش', 'کنیم', 'اثر', 'دادن', 'برخی', 'چند', 'ندارد', 'می\\u200cگویند', 'عدم', 'بهتر', 'عین', 'مشخص', 'رسیدن', 'متر', 'فرد', 'وارد', 'امکان', 'گرفته_است', 'یافته', 'سی', 'بیش', 'رفت', 'زاده', 'محسوب', 'می\\u200cدهند', 'سمت', 'چگونه', 'بسیار', 'کلی', 'کمی', 'از', 'جا', 'علیه', 'آخرین', 'دهه', 'نیز', 'همچون', 'تهیه', 'کرده', 'اما', 'درباره', 'پی', 'اجرا', 'ویژه', 'باعث', 'داشته', 'تو', 'باشند', 'همیشه', 'این', 'می\\u200cگوید', 'جمعی', 'چه', 'وجود', 'می\\u200cگیرد', 'چهارم', 'می\\u200cتواند', 'اکنون', 'بوده', 'چون', 'براساس', 'نظیر', 'تعداد', 'کنندگان', 'پر', 'تغییر', 'تمامی', 'بندی', 'طرف', 'نزدیک', 'چیزی', 'خوبی', 'حل', 'نمی\\u200cشود', 'بودن', 'حالی', 'هستند', 'جمع', 'یافته_است', 'شش', 'کاملا', 'خوب', 'ع', 'داشته_باشند', 'من', 'کردند', 'ترتیب', 'آمده', 'بنابراین', 'شده_اند', 'دیگران', 'شاید', 'دارند', 'رشد', 'سالهای', 'یکی', 'مانند', 'آیا', 'زیادی', 'نیست', 'خاطرنشان', 'دهند', 'غیر', 'کامل', 'می\\u200cدهد', 'اغلب', 'امر', 'می\\u200cکنم', 'سعی', 'همواره', 'اخیر', 'هستیم', 'شدند', 'بیشتری', 'که', 'همه', 'یابد', 'کنم', 'دادند', 'کسانی', 'در', 'می\\u200cشود', 'ساله', 'برداری', 'اند', 'هم', 'یافت', 'می\\u200cکنیم', 'آورد', 'قبل', 'بیرون', 'می\\u200cتوان', 'کوچک', 'اولین', 'روند', 'سبب', 'حدود', 'لازم', 'نیاز', 'مهم', 'بعد', 'سازی', 'سوی', 'نخست', 'ساز', 'داد', 'همان', 'دارای', 'دسته', 'چهار', 'پیدا', 'کم', 'است', 'دوم', 'رو', 'دو', 'تنها', 'شروع', 'ایشان', 'تا', 'داده_است', 'بروز', 'طی', 'منظور', 'همین', 'بوده_است', 'بخش', 'نحوه', 'شان', 'پس', 'آمد', 'شد', 'عهده', 'لذا', 'نوع', 'آنچه', 'صرف', 'کرده_بود', 'اگر', 'بود', 'گفت', 'خطر', 'همچنین', 'طبق', 'افرادی', 'مناسب', 'او', 'گروهی', 'نظر', 'کنید', 'می\\u200cرسد', 'درون', 'قابل', 'خاص', 'دچار', 'گونه', 'فوق', 'جایی', 'کردم', 'شده_بود', 'شده', 'کرده_است', 'بیان', 'شدن', 'دار', 'پیش', 'ممکن', 'تعیین', 'به', 'کسی'})\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['آید', 'توان', 'تواند', 'توانند', 'رسد', 'رود', 'سال', 'نمی', 'های', 'گوید', 'گویند'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"iuSIn5DgVsDT"},"source":["genre = list(data['Genre'])\n","labeles = [(int(i/100)-1) for i in genre]\n","data['Genre'] = labeles"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PLuHfVuqZY0T"},"source":["**RANDOM FOREST**"]},{"cell_type":"code","metadata":{"id":"34PONtLXU4X1"},"source":["X=x  # Features\n","y=data['Genre']  # Labels\n","\n","# Split dataset into training set and test set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ObvhAGAgU-c2"},"source":["#Import Random Forest Model\n","from sklearn.ensemble import RandomForestClassifier\n","\n","#Create a Gaussian Classifier\n","clf=RandomForestClassifier(n_estimators=100)\n","\n","#Train the model using the training sets y_pred=clf.predict(X_test)\n","clf.fit(X_train,y_train)\n","\n","y_pred=clf.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUvzjPc2VEbR"},"source":["#Import scikit-learn metrics module for accuracy calculation\n","from sklearn import metrics\n","# Model Accuracy, how often is the classifier correct?\n","print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yyTk-g7eaK9U"},"source":["x[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w_ntdei760k7"},"source":[""],"execution_count":null,"outputs":[]}]}